---
title: "Methodology"
description: "How ExcelBench generates fixtures, runs tests, and verifies correctness."
---

## Oracle Strategy

ExcelBench uses **Excel itself** as the ground truth.

Fixtures are generated by writing each feature through `xlwings` (Python-to-Excel automation), then letting Excel save the file. This produces `.xlsx` files that any conforming reader should be able to parse correctly.

When Excel isn't available (CI, headless), the fallback oracle is `openpyxl` — the most widely-used and battle-tested Python Excel library.

```
Write via xlwings → Excel saves → Fixture .xlsx
                                       ↓
                              Adapter reads fixture
                                       ↓
                           Compare to oracle output
```

## Feature Generation

Each feature has a dedicated generator in `src/excelbench/generator/features/`. Generators are subclasses of `FeatureGenerator` and produce a deterministic worksheet with:

1. A set of **basic** test cases (must-pass)
2. A set of **edge** test cases (bonus)

## Scoring Model

Each feature x library pair receives a score of 0–3:

| Score | Meaning |
|-------|---------|
| 3 | All test cases pass |
| 2 | ≥80% of test cases pass |
| 1 | ≥50% of test cases pass |
| 0 | below 50% pass |
| N/A | Feature not applicable (e.g. pivot_tables on macOS) |

Scores are computed separately for **read** and **write** operations. The dashboard shows the best of the two.

## Reproducibility

Every benchmark run is deterministic given the same fixture files and adapter versions. The fixtures in `fixtures/excel/` are tracked in git, so any contributor can reproduce results without Excel installed.

## Read vs. Write

Most adapters are tested for both read and write. Read-only adapters (calamine, xlrd) only receive read scores. Write-only adapters (xlsxwriter) only receive write scores.

The fidelity score on the radar chart uses `max(read_score, write_score)` to reflect the library's best capability.
