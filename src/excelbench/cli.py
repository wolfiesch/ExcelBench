"""Command-line interface for ExcelBench."""

from pathlib import Path
from typing import TYPE_CHECKING, Any

import typer
from rich.console import Console
from rich.table import Table

from excelbench.models import FeatureScore

if TYPE_CHECKING:
    from excelbench.models import BenchmarkResults

app = typer.Typer(
    name="excelbench",
    help="Comprehensive benchmark suite for Excel library feature parity.",
)
console = Console()
XLS_PROFILE_DEFAULT_TEST_DIR = Path("fixtures/excel_xls")
XLSX_PROFILE_DEFAULT_TEST_DIR = Path("test_files")
PERF_XLSX_PROFILE_DEFAULT_TEST_DIR = Path("fixtures/excel")


@app.command()
def generate(
    output_dir: Path = typer.Option(
        Path("test_files"),
        "--output",
        "-o",
        help="Directory to save generated test files.",
    ),
    features: list[str] | None = typer.Option(
        None,
        "--feature",
        "-f",
        help="Generate only the specified feature(s).",
    ),
) -> None:
    """Generate test Excel files using xlwings.

    This requires Excel to be installed and will open Excel temporarily
    to create the test files.
    """
    from excelbench.generator.generate import generate_all

    console.print(f"[bold]Generating test files to {output_dir}...[/bold]")
    console.print()

    try:
        manifest = generate_all(output_dir, features=features)

        console.print()
        console.print(f"[green]✓ Generated {len(manifest.files)} test files[/green]")

        # Show summary table
        table = Table(title="Generated Files")
        table.add_column("File", style="cyan")
        table.add_column("Feature", style="magenta")
        table.add_column("Test Cases", justify="right", style="green")

        for f in manifest.files:
            table.add_row(f.path, f.feature, str(len(f.test_cases)))

        console.print(table)

    except Exception as e:
        console.print(f"[red]Error: {e}[/red]")
        raise typer.Exit(1)


@app.command()
def benchmark(
    test_dir: Path = typer.Option(
        Path("test_files"),
        "--tests",
        "-t",
        help="Directory containing test files and manifest.json.",
    ),
    output_dir: Path = typer.Option(
        Path("results"),
        "--output",
        "-o",
        help="Directory to save benchmark results.",
    ),
    features: list[str] | None = typer.Option(
        None,
        "--feature",
        "-f",
        help="Run benchmark only for specified feature(s).",
    ),
    append_results: bool = typer.Option(
        False,
        "--append",
        "--append-results",
        help="Append into existing results.json if present.",
    ),
    profile: str = typer.Option(
        "xlsx",
        "--profile",
        help="Benchmark profile: xlsx (default) or xls.",
    ),
) -> None:
    """Run benchmark against all adapters.

    Reads the test files generated by the 'generate' command and
    tests each library adapter against them.
    """
    from excelbench.harness.adapters import get_all_adapters
    from excelbench.harness.runner import run_benchmark
    from excelbench.models import BenchmarkResults
    from excelbench.results import render_results

    profile = profile.strip().lower()
    if profile not in {"xlsx", "xls"}:
        console.print("[red]Error: profile must be one of: xlsx, xls[/red]")
        raise typer.Exit(1)

    if profile == "xls" and test_dir.resolve() == XLSX_PROFILE_DEFAULT_TEST_DIR.resolve():
        test_dir = XLS_PROFILE_DEFAULT_TEST_DIR

    adapters = None
    if profile == "xls":
        adapters = [
            adapter
            for adapter in get_all_adapters()
            if adapter.supports_read_path(Path("profile_input.xls"))
        ]
        if not adapters:
            console.print("[red]Error: No adapters available for .xls profile.[/red]")
            raise typer.Exit(1)

    console.print("[bold]Running benchmark...[/bold]")
    console.print(f"  Profile: {profile}")
    console.print(f"  Test files: {test_dir}")
    console.print(f"  Output: {output_dir}")
    console.print()

    try:
        results = run_benchmark(test_dir, adapters=adapters, features=features, profile=profile)

        if append_results:
            import json

            results_path = Path(output_dir) / "results.json"
            if results_path.exists():
                with open(results_path) as f:
                    data = json.load(f)
                existing = _results_from_json(data)
                if existing.metadata.profile != results.metadata.profile:
                    raise ValueError(
                        "Cannot append results from a different profile "
                        f"({existing.metadata.profile} vs {results.metadata.profile})."
                    )
                merged_scores = {(s.feature, s.library): s for s in existing.scores}
                for score in results.scores:
                    merged_scores[(score.feature, score.library)] = score
                results = BenchmarkResults(
                    metadata=results.metadata,
                    libraries={**existing.libraries, **results.libraries},
                    scores=list(merged_scores.values()),
                )

        console.print()
        console.print("[bold]Rendering results...[/bold]")

        render_results(results, output_dir)

        console.print()
        console.print(f"[green]✓ Results written to {output_dir}[/green]")
        console.print(f"  - {output_dir}/results.json")
        console.print(f"  - {output_dir}/README.md")
        console.print(f"  - {output_dir}/matrix.csv")

        # Show quick summary
        console.print()
        show_summary(results)

    except FileNotFoundError as e:
        console.print(f"[red]Error: {e}[/red]")
        console.print("[yellow]Did you run 'excelbench generate' first?[/yellow]")
        raise typer.Exit(1)
    except Exception as e:
        console.print(f"[red]Error: {e}[/red]")
        raise typer.Exit(1)


@app.command()
def perf(
    test_dir: Path = typer.Option(
        PERF_XLSX_PROFILE_DEFAULT_TEST_DIR,
        "--tests",
        "-t",
        help="Directory containing test files and manifest.json.",
    ),
    output_dir: Path = typer.Option(
        Path("results"),
        "--output",
        "-o",
        help="Root directory to save performance results (writes into <output>/perf).",
    ),
    features: list[str] | None = typer.Option(
        None,
        "--feature",
        "-f",
        help="Run performance benchmark only for specified feature(s).",
    ),
    adapters: list[str] | None = typer.Option(
        None,
        "--adapter",
        "-a",
        help="Run only specified adapter(s) by name (repeatable).",
    ),
    warmup: int = typer.Option(
        3,
        "--warmup",
        help="Warmup iterations to discard per (library, feature, operation).",
    ),
    iters: int = typer.Option(
        25,
        "--iters",
        help="Recorded iterations per (library, feature, operation).",
    ),
    breakdown: bool = typer.Option(
        False,
        "--breakdown",
        help="Collect phase breakdown timings (open/exercise/save, etc.).",
    ),
    profile: str = typer.Option(
        "xlsx",
        "--profile",
        help="Benchmark profile: xlsx (default) or xls.",
    ),
) -> None:
    """Run performance benchmarks (speed + best-effort memory).

    This command measures the library under test only. In particular, write timings
    do NOT include oracle verification.
    """

    from excelbench.harness.adapters import get_all_adapters
    from excelbench.perf import render_perf_results, run_perf

    profile = profile.strip().lower()
    if profile not in {"xlsx", "xls"}:
        console.print("[red]Error: profile must be one of: xlsx, xls[/red]")
        raise typer.Exit(1)

    if profile == "xls" and test_dir.resolve() == PERF_XLSX_PROFILE_DEFAULT_TEST_DIR.resolve():
        test_dir = XLS_PROFILE_DEFAULT_TEST_DIR

    available = get_all_adapters()
    selected = available

    if adapters:
        wanted = [a.strip() for a in adapters if a.strip()]
        by_name = {a.name: a for a in available}
        missing = [a for a in wanted if a not in by_name]
        if missing:
            console.print(f"[red]Error: Unknown adapters: {', '.join(missing)}[/red]")
            console.print(
                f"[yellow]Available adapters: {', '.join(sorted(by_name.keys()))}[/yellow]"
            )
            raise typer.Exit(1)
        selected = [by_name[name] for name in wanted]

    console.print("[bold]Running performance benchmark...[/bold]")
    console.print(f"  Profile: {profile}")
    console.print(f"  Test files: {test_dir}")
    console.print(f"  Output: {output_dir}/perf")
    console.print(f"  Warmup: {warmup}")
    console.print(f"  Iterations: {iters}")
    console.print(f"  Breakdown: {breakdown}")
    if adapters:
        console.print(f"  Adapters: {', '.join([a.name for a in selected])}")
    console.print()

    try:
        perf_results = run_perf(
            test_dir,
            adapters=selected,
            features=features,
            profile=profile,
            warmup=warmup,
            iters=iters,
            breakdown=breakdown,
        )
        render_perf_results(perf_results, output_dir)

        console.print(f"[green]✓ Performance results written to {output_dir}/perf[/green]")
        console.print(f"  - {output_dir}/perf/results.json")
        console.print(f"  - {output_dir}/perf/README.md")
        console.print(f"  - {output_dir}/perf/matrix.csv")
        console.print(f"  - {output_dir}/perf/history.jsonl")

    except FileNotFoundError as e:
        console.print(f"[red]Error: {e}[/red]")
        raise typer.Exit(1)
    except Exception as e:
        console.print(f"[red]Error: {e}[/red]")
        raise typer.Exit(1)


@app.command("generate-xls")
def generate_xls_command(
    output_dir: Path = typer.Option(
        XLS_PROFILE_DEFAULT_TEST_DIR,
        "--output",
        "-o",
        help="Directory to save generated .xls test files.",
    ),
    features: list[str] | None = typer.Option(
        None,
        "--feature",
        "-f",
        help="Generate only the specified .xls feature(s).",
    ),
) -> None:
    """Generate deterministic .xls fixtures for the .xls benchmark profile."""
    from excelbench.generator.generate_xls import generate_xls

    console.print(f"[bold]Generating .xls test files to {output_dir}...[/bold]")
    console.print()

    try:
        manifest = generate_xls(output_dir, features=features)
        console.print(f"[green]✓ Generated {len(manifest.files)} .xls files[/green]")

        table = Table(title="Generated .xls Files")
        table.add_column("File", style="cyan")
        table.add_column("Feature", style="magenta")
        table.add_column("Test Cases", justify="right", style="green")
        for test_file in manifest.files:
            table.add_row(test_file.path, test_file.feature, str(len(test_file.test_cases)))
        console.print(table)
    except Exception as e:
        console.print(f"[red]Error: {e}[/red]")
        raise typer.Exit(1)


@app.command("benchmark-profiles")
def benchmark_profiles(
    xlsx_tests: Path = typer.Option(
        XLSX_PROFILE_DEFAULT_TEST_DIR,
        "--xlsx-tests",
        help="Directory containing .xlsx tests and manifest.json.",
    ),
    xls_tests: Path = typer.Option(
        XLS_PROFILE_DEFAULT_TEST_DIR,
        "--xls-tests",
        help="Directory containing .xls tests and manifest.json.",
    ),
    output_dir: Path = typer.Option(
        Path("results"),
        "--output",
        "-o",
        help="Root directory for split profile results.",
    ),
) -> None:
    """Run both xlsx and xls profiles and render split outputs."""
    from excelbench.harness.adapters import get_all_adapters
    from excelbench.harness.runner import run_benchmark
    from excelbench.results import render_results

    output_dir = Path(output_dir)
    xlsx_output = output_dir / "xlsx"
    xls_output = output_dir / "xls"

    console.print("[bold]Running xlsx profile...[/bold]")
    xlsx_results = run_benchmark(xlsx_tests, profile="xlsx")
    render_results(xlsx_results, xlsx_output)

    console.print("[bold]Running xls profile...[/bold]")
    xls_adapters = [
        adapter
        for adapter in get_all_adapters()
        if adapter.supports_read_path(Path("profile_input.xls"))
    ]
    if not xls_adapters:
        console.print("[red]Error: No adapters available for .xls profile.[/red]")
        raise typer.Exit(1)
    xls_results = run_benchmark(xls_tests, adapters=xls_adapters, profile="xls")
    render_results(xls_results, xls_output)

    _write_profile_index(output_dir)

    console.print(f"[green]✓ Wrote split results to {output_dir}[/green]")
    console.print(f"  - {xlsx_output}/results.json")
    console.print(f"  - {xls_output}/results.json")
    console.print(f"  - {output_dir}/README.md")


def _results_from_json(data: dict[str, Any]) -> "BenchmarkResults":
    from datetime import datetime

    from excelbench.models import (
        BenchmarkMetadata,
        BenchmarkResults,
        Diagnostic,
        DiagnosticCategory,
        DiagnosticLocation,
        DiagnosticSeverity,
        FeatureScore,
        Importance,
        LibraryInfo,
        OperationType,
        TestResult,
    )

    metadata = BenchmarkMetadata(
        benchmark_version=data["metadata"]["benchmark_version"],
        run_date=datetime.fromisoformat(data["metadata"]["run_date"]),
        excel_version=data["metadata"]["excel_version"],
        platform=data["metadata"]["platform"],
        profile=data["metadata"].get("profile", "xlsx"),
    )

    libraries = {
        name: LibraryInfo(
            name=info["name"],
            version=info["version"],
            language=info["language"],
            capabilities=set(info["capabilities"]),
        )
        for name, info in data["libraries"].items()
    }

    scores = []
    for s in data["results"]:
        test_results = []
        for tc_id, tc in s.get("test_cases", {}).items():
            # New schema: { "read": {...}, "write": {...} }
            if isinstance(tc, dict) and ("read" in tc or "write" in tc):
                for op_key in ("read", "write"):
                    if op_key not in tc:
                        continue
                    op_data = tc[op_key]
                    test_results.append(
                        TestResult(
                            test_case_id=tc_id,
                            operation=OperationType(op_key),
                            passed=op_data["passed"],
                            expected=op_data["expected"],
                            actual=op_data["actual"],
                            notes=op_data.get("notes"),
                            diagnostics=[
                                Diagnostic(
                                    category=DiagnosticCategory(d["category"]),
                                    severity=DiagnosticSeverity(d["severity"]),
                                    location=DiagnosticLocation(
                                        feature=d["location"]["feature"],
                                        operation=OperationType(d["location"]["operation"]),
                                        test_case_id=d["location"].get("test_case_id"),
                                        sheet=d["location"].get("sheet"),
                                        cell=d["location"].get("cell"),
                                    ),
                                    adapter_message=d["adapter_message"],
                                    probable_cause=d.get("probable_cause"),
                                )
                                for d in op_data.get("diagnostics", [])
                            ],
                            importance=(
                                Importance(op_data["importance"])
                                if op_data.get("importance")
                                else None
                            ),
                            label=op_data.get("label"),
                        )
                    )
            else:
                # Legacy schema: flat per test case, assume read
                test_results.append(
                    TestResult(
                        test_case_id=tc_id,
                        operation=OperationType.READ,
                        passed=tc["passed"],
                        expected=tc["expected"],
                        actual=tc["actual"],
                        notes=tc.get("notes"),
                        diagnostics=[],
                        importance=(Importance(tc["importance"]) if tc.get("importance") else None),
                        label=tc.get("label"),
                    )
                )
        scores.append(
            FeatureScore(
                feature=s["feature"],
                library=s["library"],
                read_score=s["scores"].get("read"),
                write_score=s["scores"].get("write"),
                test_results=test_results,
                notes=s.get("notes"),
            )
        )

    for score in scores:
        if score.notes:
            continue
        if (
            score.feature == "pivot_tables"
            and metadata.platform.startswith("Darwin")
            and score.read_score is None
            and score.write_score is None
        ):
            score.notes = (
                "Unsupported on macOS without a Windows-generated pivot fixture "
                "(fixtures/excel/tier2/15_pivot_tables.xlsx)."
            )

    return BenchmarkResults(
        metadata=metadata,
        libraries=libraries,
        scores=scores,
    )


def _write_profile_index(output_dir: Path) -> None:
    output_dir.mkdir(parents=True, exist_ok=True)
    content = "\n".join(
        [
            "# ExcelBench Results",
            "",
            "Results are split by workbook format profile.",
            "",
            "- [xlsx profile](./xlsx/README.md)",
            "- [xls profile](./xls/README.md)",
            "",
            "> Do not compare scores across these profiles directly. "
            "They target different workbook formats and adapter capability surfaces.",
            "",
        ]
    )
    (output_dir / "README.md").write_text(content)


@app.command()
def report(
    results_path: Path = typer.Option(
        Path("results/results.json"),
        "--input",
        "-i",
        help="Path to results.json file.",
    ),
    output_dir: Path = typer.Option(
        Path("results"),
        "--output",
        "-o",
        help="Directory to save regenerated reports.",
    ),
) -> None:
    """Regenerate reports from existing results.json.

    Useful for updating markdown/CSV without re-running the benchmark.
    """
    import json

    from excelbench.results import render_csv, render_markdown

    console.print(f"[bold]Regenerating reports from {results_path}...[/bold]")

    try:
        with open(results_path) as f:
            data = json.load(f)

        results = _results_from_json(data)

        # Regenerate reports
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        render_markdown(results, output_dir / "README.md")
        render_csv(results, output_dir / "matrix.csv")

        console.print("[green]✓ Reports regenerated[/green]")

    except Exception as e:
        console.print(f"[red]Error: {e}[/red]")
        raise typer.Exit(1)


@app.command()
def heatmap(
    results_path: Path = typer.Option(
        Path("results/xlsx/results.json"),
        "--input",
        "-i",
        help="Path to fidelity results.json file.",
    ),
    output_dir: Path = typer.Option(
        Path("results/xlsx"),
        "--output",
        "-o",
        help="Directory to save heatmap images.",
    ),
) -> None:
    """Generate heatmap visualization (PNG + SVG) from fidelity results."""
    from excelbench.results.heatmap import render_heatmap

    console.print(f"[bold]Generating heatmap from {results_path}...[/bold]")

    try:
        paths = render_heatmap(results_path, output_dir)
        for p in paths:
            console.print(f"  [green]✓[/green] {p}")
        if not paths:
            console.print("[yellow]No scored results found — nothing to render.[/yellow]")
    except Exception as e:
        console.print(f"[red]Error: {e}[/red]")
        raise typer.Exit(1)


@app.command()
def dashboard(
    fidelity_path: Path = typer.Option(
        Path("results/xlsx/results.json"),
        "--fidelity",
        "-f",
        help="Path to fidelity results.json.",
    ),
    perf_path: Path = typer.Option(
        Path("results/perf/results.json"),
        "--perf",
        "-p",
        help="Path to performance results.json (optional).",
    ),
    output_path: Path = typer.Option(
        Path("results/DASHBOARD.md"),
        "--output",
        "-o",
        help="Output markdown file path.",
    ),
) -> None:
    """Generate combined fidelity + performance dashboard."""
    from excelbench.results.dashboard import render_dashboard

    console.print("[bold]Generating dashboard...[/bold]")

    try:
        perf = perf_path if perf_path.exists() else None
        render_dashboard(fidelity_path, perf, output_path)
        console.print(f"  [green]✓[/green] {output_path}")
    except Exception as e:
        console.print(f"[red]Error: {e}[/red]")
        raise typer.Exit(1)


def show_summary(results: "BenchmarkResults") -> None:
    """Show a quick summary table of results."""
    from excelbench.results.renderer import score_emoji

    table = Table(title="Benchmark Summary")
    table.add_column("Feature", style="cyan")

    # Add library columns
    libraries = sorted(results.libraries.keys())
    for lib in libraries:
        caps = results.libraries[lib].capabilities
        if "read" in caps and "write" in caps:
            table.add_column(f"{lib} (R)", justify="center")
            table.add_column(f"{lib} (W)", justify="center")
        elif "read" in caps:
            table.add_column(f"{lib} (R)", justify="center")
        else:
            table.add_column(f"{lib} (W)", justify="center")

    # Build lookup
    score_lookup: dict[tuple[str, str], FeatureScore] = {}
    for score_entry in results.scores:
        score_lookup[(score_entry.feature, score_entry.library)] = score_entry

    # Add rows
    features = sorted(set(s.feature for s in results.scores))
    for feature in features:
        row = [feature]
        for lib in libraries:
            score = score_lookup.get((feature, lib))
            caps = results.libraries[lib].capabilities

            if "read" in caps:
                if score and score.read_score is not None:
                    row.append(score_emoji(score.read_score))
                else:
                    row.append("➖")

            if "write" in caps:
                if score and score.write_score is not None:
                    row.append(score_emoji(score.write_score))
                else:
                    row.append("➖")

        table.add_row(*row)

    console.print(table)


if __name__ == "__main__":
    app()
