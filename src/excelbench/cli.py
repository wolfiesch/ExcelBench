"""Command-line interface for ExcelBench."""

from pathlib import Path

import typer
from rich.console import Console
from rich.table import Table

app = typer.Typer(
    name="excelbench",
    help="Comprehensive benchmark suite for Excel library feature parity.",
)
console = Console()


@app.command()
def generate(
    output_dir: Path = typer.Option(
        Path("test_files"),
        "--output", "-o",
        help="Directory to save generated test files.",
    ),
):
    """Generate test Excel files using xlwings.

    This requires Excel to be installed and will open Excel temporarily
    to create the test files.
    """
    from excelbench.generator.generate import generate_all

    console.print(f"[bold]Generating test files to {output_dir}...[/bold]")
    console.print()

    try:
        manifest = generate_all(output_dir)

        console.print()
        console.print(f"[green]✓ Generated {len(manifest.files)} test files[/green]")

        # Show summary table
        table = Table(title="Generated Files")
        table.add_column("File", style="cyan")
        table.add_column("Feature", style="magenta")
        table.add_column("Test Cases", justify="right", style="green")

        for f in manifest.files:
            table.add_row(f.path, f.feature, str(len(f.test_cases)))

        console.print(table)

    except Exception as e:
        console.print(f"[red]Error: {e}[/red]")
        raise typer.Exit(1)


@app.command()
def benchmark(
    test_dir: Path = typer.Option(
        Path("test_files"),
        "--tests", "-t",
        help="Directory containing test files and manifest.json.",
    ),
    output_dir: Path = typer.Option(
        Path("results"),
        "--output", "-o",
        help="Directory to save benchmark results.",
    ),
):
    """Run benchmark against all adapters.

    Reads the test files generated by the 'generate' command and
    tests each library adapter against them.
    """
    from excelbench.harness.runner import run_benchmark
    from excelbench.results import render_results

    console.print(f"[bold]Running benchmark...[/bold]")
    console.print(f"  Test files: {test_dir}")
    console.print(f"  Output: {output_dir}")
    console.print()

    try:
        results = run_benchmark(test_dir)

        console.print()
        console.print("[bold]Rendering results...[/bold]")

        render_results(results, output_dir)

        console.print()
        console.print(f"[green]✓ Results written to {output_dir}[/green]")
        console.print(f"  - {output_dir}/results.json")
        console.print(f"  - {output_dir}/README.md")
        console.print(f"  - {output_dir}/matrix.csv")

        # Show quick summary
        console.print()
        show_summary(results)

    except FileNotFoundError as e:
        console.print(f"[red]Error: {e}[/red]")
        console.print("[yellow]Did you run 'excelbench generate' first?[/yellow]")
        raise typer.Exit(1)
    except Exception as e:
        console.print(f"[red]Error: {e}[/red]")
        raise typer.Exit(1)


@app.command()
def report(
    results_path: Path = typer.Option(
        Path("results/results.json"),
        "--input", "-i",
        help="Path to results.json file.",
    ),
    output_dir: Path = typer.Option(
        Path("results"),
        "--output", "-o",
        help="Directory to save regenerated reports.",
    ),
):
    """Regenerate reports from existing results.json.

    Useful for updating markdown/CSV without re-running the benchmark.
    """
    import json
    from datetime import datetime
    from excelbench.models import (
        BenchmarkResults,
        BenchmarkMetadata,
        LibraryInfo,
        FeatureScore,
        TestResult,
    )
    from excelbench.results import render_markdown, render_csv

    console.print(f"[bold]Regenerating reports from {results_path}...[/bold]")

    try:
        with open(results_path) as f:
            data = json.load(f)

        # Reconstruct results object
        metadata = BenchmarkMetadata(
            benchmark_version=data["metadata"]["benchmark_version"],
            run_date=datetime.fromisoformat(data["metadata"]["run_date"]),
            excel_version=data["metadata"]["excel_version"],
            platform=data["metadata"]["platform"],
        )

        libraries = {
            name: LibraryInfo(
                name=info["name"],
                version=info["version"],
                language=info["language"],
                capabilities=set(info["capabilities"]),
            )
            for name, info in data["libraries"].items()
        }

        scores = []
        for s in data["results"]:
            test_results = [
                TestResult(
                    test_case_id=tc_id,
                    passed=tc["passed"],
                    expected=tc["expected"],
                    actual=tc["actual"],
                    notes=tc.get("notes"),
                )
                for tc_id, tc in s.get("test_cases", {}).items()
            ]
            scores.append(FeatureScore(
                feature=s["feature"],
                library=s["library"],
                read_score=s["scores"].get("read"),
                write_score=s["scores"].get("write"),
                test_results=test_results,
                notes=s.get("notes"),
            ))

        results = BenchmarkResults(
            metadata=metadata,
            libraries=libraries,
            scores=scores,
        )

        # Regenerate reports
        output_dir = Path(output_dir)
        render_markdown(results, output_dir / "README.md")
        render_csv(results, output_dir / "matrix.csv")

        console.print(f"[green]✓ Reports regenerated[/green]")

    except Exception as e:
        console.print(f"[red]Error: {e}[/red]")
        raise typer.Exit(1)


def show_summary(results):
    """Show a quick summary table of results."""
    from excelbench.results.renderer import score_emoji

    table = Table(title="Benchmark Summary")
    table.add_column("Feature", style="cyan")

    # Add library columns
    libraries = sorted(results.libraries.keys())
    for lib in libraries:
        caps = results.libraries[lib].capabilities
        if "read" in caps and "write" in caps:
            table.add_column(f"{lib} (R)", justify="center")
            table.add_column(f"{lib} (W)", justify="center")
        elif "read" in caps:
            table.add_column(f"{lib} (R)", justify="center")
        else:
            table.add_column(f"{lib} (W)", justify="center")

    # Build lookup
    score_lookup = {}
    for score in results.scores:
        score_lookup[(score.feature, score.library)] = score

    # Add rows
    features = sorted(set(s.feature for s in results.scores))
    for feature in features:
        row = [feature]
        for lib in libraries:
            score = score_lookup.get((feature, lib))
            caps = results.libraries[lib].capabilities

            if "read" in caps:
                if score and score.read_score is not None:
                    row.append(score_emoji(score.read_score))
                else:
                    row.append("➖")

            if "write" in caps:
                if score and score.write_score is not None:
                    row.append(score_emoji(score.write_score))
                else:
                    row.append("➖")

        table.add_row(*row)

    console.print(table)


if __name__ == "__main__":
    app()
